{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.12.1 64-bit' requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# PART 1\n",
    "import requests #We are using this library to request the content of a web page by sending a GET request to the provided URL\n",
    "from bs4 import BeautifulSoup #we are using this as it creates parse trees that basically organise and almost filter the data of the website that we are trying to extract from\n",
    "import pandas as pd # we are using it to organise the data from the recipe to store present into table columns\n",
    "import json #this will allows us to take certain information from recipe that is basically easy for the program use \n",
    "\n",
    "# We are configure pandas display options to ensure all columns and rows are displayed without being cut short\n",
    "pd.set_option('display.max_columns', None)# this ensures that no matter how many columns there are every single one will be shown \n",
    "pd.set_option('display.width', None)#this makes sure that each line per row in that table will be given enough room to display the data\n",
    "pd.set_option('display.max_colwidth', None)#this makes sure that all the information will be displayed no matter how long it is\n",
    "pd.set_option('display.max_rows', None)# this basically makes sure all rows are displayed as some rows maybe longer than others\n",
    "\n",
    "def collect_page_data(url):# our defined function \n",
    "    # we are extracting the data from the webpage and we are storing it in the response variable\n",
    "    response = requests.get(url)\n",
    "    # we are using to get the data from the webpage and have it in a structure it in a way that is easy to navigate \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    script = soup.find('script', {'type': 'application/ld+json'}) # by finding this we would find the information necessary about the webpage content\n",
    "\n",
    "    data = json.loads(script.string) if script else {} # if the piece of code isn't there we are just creating an empty dictionary and still allow the code to run\n",
    "    \n",
    "    # We are extracting the recipe's specific details like the title ,cooking time,image etc.If for some reason the data is not availiable for the category we need it will say that it is not specified\n",
    "    title = data.get('name', 'Not specified')\n",
    "    # We will add the cooking and preparation times after removing the 'PT'(period time)\n",
    "    total_time = data.get(\"cookTime\").strip(\"PT\") + data.get(\"prepTime\").strip(\"PT\")\n",
    "    # We are extracting the URL of the image and the filename of the image.\n",
    "    image_url = data.get('image', ['No image'])[0]\n",
    "    image = image_url.split('/')[-1] if image_url != 'No image' else 'No image'\n",
    "    # we are adding the list of ingredients into a single string and we are seprating using commas\n",
    "    ingredients_list = data.get('recipeIngredient', [])\n",
    "    ingredients = ', '.join(ingredients_list) if ingredients_list else 'Not specified'\n",
    "    # We are extracting the rating value and count.\n",
    "    rating_val = data.get('aggregateRating', {}).get('ratingValue', 'Not available')\n",
    "    rating_count = data.get('aggregateRating', {}).get('ratingCount', 'Not available')\n",
    "    # We are extracting the category and cuisine type.\n",
    "    category = data.get('recipeCategory', 'Not specified')\n",
    "    cuisine = data.get('recipeCuisine', 'Not specified')\n",
    "\n",
    "    # We are extracting the list of suitable diets, formatted as webpages, and determine whether they are vegan/vegetarian \n",
    "    diet_list = data.get('suitableForDiet', [])\n",
    "    diet = ', '.join([d.split('/')[-1] for d in diet_list]) if diet_list else 'Not specified'\n",
    "    # We are comparing the list to the VeganDiet/VegetarianDiet webpages to set the vegan/vegetarian \n",
    "    vegan = 'Yes' if 'http://schema.org/VeganDiet' in diet_list else 'No'\n",
    "    vegetarian = 'Yes' if 'http://schema.org/VegetarianDiet' in diet_list else 'No'\n",
    "    \n",
    "    # Construct a DataFrame from the extracted data and we are creating a table with the raw data obttained by the webpage\n",
    "    #we are creating a dictionary with all of the necessary information and each infromation corresponds to a different key\n",
    "    data_df = {\n",
    "        'title': [title],\n",
    "        'total_time': [total_time], \n",
    "        'image': [image],\n",
    "        'ingredients': [ingredients],\n",
    "        'rating_val': [rating_val],\n",
    "        'rating_count': [rating_count],\n",
    "        'category': [category],\n",
    "        'cuisine': [cuisine],\n",
    "        'diet': [diet],\n",
    "        'vegan': [vegan],\n",
    "        'vegetarian': [vegetarian],\n",
    "        'url': [url]\n",
    "    }\n",
    "\n",
    "    # We are making a dataframe from the dictionary and presenting it in a table format\n",
    "    df = pd.DataFrame(data_df)\n",
    "\n",
    "    # Return the dataframe containing the recipe's data.\n",
    "    return df\n",
    "\n",
    "# We are calling the function with the specific webpage and store the result in 'df' table.\n",
    "url = 'https://www.bbc.co.uk/food/recipes/easiest_ever_banana_cake_42108'\n",
    "df = collect_page_data(url)\n",
    "\n",
    "# Printing the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#load the content of both files and combine them into one dataframe\n",
    "df1 = pd.read_csv(\"books_new.csv\")\n",
    "# df1\n",
    "\n",
    "df2 = pd.read_csv(\"ratings.csv\")\n",
    "# df2\n",
    "\n",
    "combined_df = pd.merge(df1,df2, on='bookId')\n",
    "print(combined_df)\n",
    "\n",
    "#show summary statistics\n",
    "summary_statistics = combined_df.describe() #shows summary statistics\n",
    "print(summary_statistics)\n",
    "\n",
    "#handle missing values\n",
    "missing_values = combined_df.isnull().sum() #handle missing values\n",
    "print(missing_values)\n",
    "\n",
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#calculate the average rating for each book and show top 10 books with highest ratings\n",
    "average_ratings = combined_df.groupby('bookId')['rating'].mean()#.sort_values(ascending=False)\n",
    "top10_ratings = average_ratings.nlargest(10)\n",
    "print(top10_ratings)\n",
    "#print(average_ratings.head(10))\n",
    "\n",
    "#compute a 95% confidence interval for the average ratings\n",
    "def bootstrap_mean(data, n_samples=1000, sample_size=100):\n",
    "    bootstrap_mean = np.empty(n_samples)\n",
    "    np.random.seed(42)\n",
    "    for i in range(n_samples):\n",
    "        bootstrap_sample = np.random.choice(data, size=sample_size)#combined_df['rating'].sample(n=sample_size, replace=True)\n",
    "        bootstrap_mean[i] = np.mean(bootstrap_sample)#.append(bootstrap_sample.mean())\n",
    "    return np.percentile(bootstrap_mean, [2.5, 97.5])\n",
    "\n",
    "confidence_intervals = {}\n",
    "for book_id, group in combined_df.groupby('bookId'):\n",
    "    confidence_intervals[book_id] = bootstrap_mean(group['rating'].values, 1000, 100)\n",
    "\n",
    "# Print confidence intervals for the top 10 highest average ratings\n",
    "for book_id, conf_interval in list(confidence_intervals.items())[:10]:\n",
    "    print(f\"\\nBook: {book_id}, 95% Confidence Interval for the Average Ratings: ({conf_interval})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 3\n",
    "# include an extra column called rating count and analyze the relationship\n",
    "rating_count = combined_df.groupby('bookId')['rating'].count().reset_index()\n",
    "rating_count.columns = ['bookId', 'ratingCount']\n",
    "combined_df = pd.merge(combined_df, rating_count, on='bookId')\n",
    "\n",
    "#analyze the relationship between average rating and rating count\n",
    "correlation = combined_df['rating'].corr(combined_df['ratingCount'])\n",
    "print(\"\\nCorrelation between Average Rating and Rating Count:\", correlation)\n",
    "\n",
    "# Suggest a threshold for the number of ratings under which the rating can be considered as not significant\n",
    "threshold = 50\n",
    "significant_ratings = combined_df[combined_df['ratingCount'] < threshold]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBooks with Rating Count Less Than Threshold:\")\n",
    "print(significant_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 4\n",
    "\n",
    "import pandas as pd # we are using it to organise the data from the recipe to store present into table columns\n",
    "df1 = pd.read_csv(\"books_new.csv\") #it reads books_new.csv and the contents of it will be shown on df1\n",
    "df2 = pd.read_csv(\"ratings.csv\") #it reads ratings.csv and the contents of it will be shown on df2\n",
    "combined_df = pd.merge(df1,df2, on='bookId')# we are creating a new data frame where it will combine df1 & df2 based on the bookId as both cs files have those in common\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #we are using the CountVectorizer to convert text data in numerical data\n",
    "from sklearn.metrics.pairwise import cosine_similarity#we are using the cosine_similarity to see the similarity between the books based on combined features\n",
    "\n",
    "# we are converting the rating column from the combined_df into a binary value and we called it \"like_dislike\"\n",
    "#with 1 being a like only if the rating is =>3.6 otherwise it will be a dislike(-1)\n",
    "combined_df['like_dislike'] = combined_df['rating'].apply(lambda x: 1 if x >= 3.6 else -1)\n",
    "\n",
    "# we are selecting the specific features of the book like the title,author etc.\n",
    "features_to_combine = ['Title', 'Author', 'SubGenre', 'Publisher']\n",
    "\n",
    "#this is to fill in the missing values in those columns with empty slots to those without have a nan(not a number)\n",
    "# this is to make sure that if there is a certain information missed from any books then fill in the blank with a blank space\n",
    "for feature in features_to_combine:\n",
    "    combined_df[feature] = combined_df[feature].fillna('')\n",
    "\n",
    "# we are combining the features \n",
    "combined_df['combined_features'] = combined_df[features_to_combine].agg(' '.join, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# We are setting up CountVectorizer we are in this case just converting text data into numerical data\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the combined_features to a matrix of token counts\n",
    "#we are taking all of the strings count how many times it appears the words appears in the combined features \n",
    "count_matrix = vectorizer.fit_transform(combined_df['combined_features'])\n",
    "\n",
    "# Compute the cosine similarity matrix from the count matrix\n",
    "#this calculates how similar each book is to one another  using the cosine of the angle of the books \n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "# prints the cosine similarity \n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 5\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vec_space_method(query_book, dataset):\n",
    "    #initializing Vectorizer and transforming the text data to TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['Title'])\n",
    "   \n",
    "    #Locating the dataset's query book index.\n",
    "    query_index = dataset[dataset['Title'] == query_book].index[0]\n",
    "\n",
    "    #Calculating the cosine Similarity between the book and all other books\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[query_index], tfidf_matrix).flatten()\n",
    "\n",
    "    #Obtain the top ten most similar book indexes (without including the query book itself).\n",
    "    similar_indices = cosine_similarities.argsort()[:-11:-1]\n",
    "\n",
    "    #Get the most similar books' titles and commonalities.\n",
    "    similar_books = [(dataset.iloc[i]['Title'], cosine_similarities[i]) for i in similar_indices if i != query_index]\n",
    "\n",
    "    return similar_books\n",
    "\n",
    "\n",
    "books = pd.read_csv('books_new.csv')\n",
    "\n",
    "query_book = 'Girl with the Dragon Tattoo'\n",
    "similar_books = vec_space_method(query_book, books)\n",
    "\n",
    "print(\"10 most similar books to '{}' are:\".format(query_book))\n",
    "for book, similarity in similar_books:\n",
    "    print(\"\\n-{}, Similarity: {:.2f}\".format(book, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 QUESTION 6\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def knn_similarity(query_book, dataset, k=10):\n",
    "    # Initializing Vectorizer and transforming the text data to TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['Title'])\n",
    "\n",
    "    # Locating the dataset's query book index\n",
    "    query_index = dataset[dataset['Title'] == query_book].index[0]\n",
    "\n",
    "    # Initializing KNN model\n",
    "    knn_model = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    knn_model.fit(tfidf_matrix)\n",
    "\n",
    "    # Finding k nearest neighbors for the query book\n",
    "    distances, indices = knn_model.kneighbors(tfidf_matrix[query_index], n_neighbors=k+1)\n",
    "\n",
    "    # Get the most similar books' titles and similarities\n",
    "    similar_books = [(dataset.iloc[idx]['Title'], 1 - distance) for distance, idx in zip(distances.flatten()[1:], indices.flatten()[1:])]\n",
    "\n",
    "    return similar_books\n",
    "\n",
    "# Example usage\n",
    "books = pd.read_csv('books_new.csv')\n",
    "query_book = 'Girl with the Dragon Tattoo'\n",
    "similar_books = knn_similarity(query_book, books)\n",
    "\n",
    "print(f\"10 most similar books to '{query_book}' are:\")\n",
    "for book, similarity in similar_books:\n",
    "    print(f\"\\n- {book}, Similarity: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(bk):\n",
    "    bk.dropna(axis=0, inplace=True) # Delete rows with missing values\n",
    "    bk.dropna(axis=1, how='all', inplace=True) # Delete columns with all missing values\n",
    "    return bk\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "# Function to calculate coverage\n",
    "def calculate_coverage(recommendations, total_books):\n",
    "    unique_books = set(recommendations)\n",
    "    coverage = len(unique_books) / total_books * 100\n",
    "    return coverage\n",
    "\n",
    "# Function to calculate personalization\n",
    "def calculate_personalization(recommendations):\n",
    "    num_users = len(recommendations)\n",
    "    num_unique_books = len(set(book for user_rec in recommendations for book in user_rec))\n",
    "    personalization = 1 - (num_unique_books / (num_users * len(recommendations[0])))\n",
    "    return personalization\n",
    "\n",
    "# Function to perform KNN similarity\n",
    "def knn_similarity(query_book, dataset, k=10):\n",
    "    # Initializing Vectorizer and transforming the text data to TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['Title'])\n",
    "\n",
    "    # Locating the dataset's query book index\n",
    "    query_index = dataset[dataset['Title'] == query_book].index[0]\n",
    "\n",
    "    # Initializing KNN model\n",
    "    knn_model = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "    knn_model.fit(tfidf_matrix)\n",
    "\n",
    "    # Finding k nearest neighbors for the query book\n",
    "    distances, indices = knn_model.kneighbors(tfidf_matrix[query_index], n_neighbors=k+1)\n",
    "\n",
    "    # Get the most similar books' titles and similarities\n",
    "    similar_books = [dataset.iloc[idx]['Title'] for idx in indices.flatten()[1:]]\n",
    "\n",
    "    return similar_books\n",
    "\n",
    "# Read the datasets\n",
    "books_df = pd.read_csv('books_new.csv')\n",
    "\n",
    "# Clean the book dataset\n",
    "books_df = clean_data(books_df)\n",
    "\n",
    "# Initialize the tfidf_vectorizer and tfidf_matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(books_df['Title'])\n",
    "\n",
    "# Define user preferences test set\n",
    "user_preferences_test = {\n",
    "    1: 'Fundamentals of Wavelets', \n",
    "    2: 'Orientalism', \n",
    "    3: 'How to Think Like Sherlock Holmes', \n",
    "    4: 'Data Scientists at Work'\n",
    "}\n",
    "\n",
    "# Initialize lists to store recommendations for each user from both methods\n",
    "vector_space_recommendations = []\n",
    "knn_recommendations = []\n",
    "\n",
    "# Initialize KNN model\n",
    "knn_model = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "knn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Iterate over each user and their preferred book\n",
    "for user_id, preferred_book in user_preferences_test.items():\n",
    "    \n",
    "    # Get recommendations using the vector space method for the preferred book\n",
    "    query_index = books_df[books_df['Title'] == preferred_book].index[0]\n",
    "    distances, indices = knn_model.kneighbors(tfidf_matrix[query_index])\n",
    "    similar_books = [books_df.iloc[idx]['Title'] for idx in indices.flatten()[1:]]\n",
    "    vector_space_recommendations.append(similar_books)\n",
    "\n",
    "    # Get recommendations using the KNN method for the preferred book\n",
    "    knn_rec = knn_similarity(preferred_book, books_df)\n",
    "    knn_recommendations.append(knn_rec)\n",
    "\n",
    "# Calculate Coverage for Vector Space Method\n",
    "total_unique_books = len(books_df)  # Total number of unique books in the dataset\n",
    "vector_space_coverage = calculate_coverage([book for sublist in vector_space_recommendations for book in sublist], total_unique_books)\n",
    "\n",
    "# Calculate Coverage for KNN Method\n",
    "knn_coverage = calculate_coverage([book for sublist in knn_recommendations for book in sublist], total_unique_books)\n",
    "\n",
    "# Print Coverages for each model\n",
    "print(\"Vector Space Method Coverage:\", vector_space_coverage, \"%\")\n",
    "print(\"KNN Method Coverage:\", knn_coverage, \"%\")\n",
    "\n",
    "# Calculate Personalization for Vector Space Method\n",
    "personalization_vector_space = calculate_personalization(vector_space_recommendations)\n",
    "\n",
    "# Calculate Personalization for KNN Method\n",
    "personalization_knn = calculate_personalization(knn_recommendations)\n",
    "\n",
    "# Print Personalization Scores\n",
    "print(\"Vector Space Method Personalization:\", personalization_vector_space)\n",
    "print(\"KNN Method Personalization:\", personalization_knn)\n",
    "\n",
    "# Analysis Comments: # Review of Coverage: # The coverage of each approach varies. In comparison to the KNN Method, the Vector Space Method covers a larger spectrum of unique publications. This implies that a wider range of recommendations are provided by the Vector Space Method for different consumers.\n",
    "\n",
    "# Evaluation of Personalisation: # When compared to the KNN Method, the Vector Space Method shows more personalisation. This suggests that there is less overlap between recommended lists for various users since the recommendations made by the Vector Space Method are more individualised to each user's interests. On the other hand, the KNN Method displays less personalisation, indicating that each user's recommendations are not as customised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Your existing code\n",
    "# Read the datasets\n",
    "bk = pd.read_csv('books_new.csv')\n",
    "rt = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(bk):\n",
    "    bk.dropna(axis=0, inplace=True) # Delete rows with missing values\n",
    "    bk.dropna(axis=1, how='all', inplace=True) # Delete columns with all missing values\n",
    "    return bk\n",
    "\n",
    "# Clean the book dataset\n",
    "bk = clean_data(bk)\n",
    "\n",
    "# Perform one-hot encoding on categorical columns\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "bk_encoded = encoder.fit_transform(bk[['Title', 'Author', 'Genre']])  # Encode 'title', 'author', and 'genre' columns\n",
    "encoded_columns = encoder.get_feature_names_out(['Title', 'Author', 'Genre'])\n",
    "bk_encoded = pd.DataFrame(bk_encoded, columns=encoded_columns)\n",
    "\n",
    "# Merge datasets based on a common key (e.g., book ID)\n",
    "merged_data = pd.concat([bk_encoded, rt], axis=1, join='inner')\n",
    "\n",
    "# Preprocess the merged dataset and prepare features and target variable\n",
    "X = merged_data.drop(columns=['user_id', 'rating'])  # Features\n",
    "y = merged_data['rating']  # Target variable\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "knn_model.fit(X)\n",
    "\n",
    "# Function to predict whether a user would like a book\n",
    "def predict_like(query_book_features):\n",
    "    _, indices = knn_model.kneighbors(query_book_features)\n",
    "    nearest_neighbor_ratings = y.iloc[indices.flatten()]  # Ratings of nearest neighbors\n",
    "    \n",
    "    # Predict whether the user would like the query book\n",
    "    prediction = np.mean(nearest_neighbor_ratings) >= 3  # If mean rating is 3 or higher, predict 'like', else 'dislike'\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Reset the index of X DataFrame\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Iterate over each unique user ID in the ratings dataset and make predictions\n",
    "unique_user_ids = rt['user_id'].unique()\n",
    "for user_id in unique_user_ids:\n",
    "    # Get all books rated by the current user\n",
    "    user_books = rt[rt['user_id'] == user_id]['bookId']\n",
    "    \n",
    "    # Iterate over each book rated by the user and make predictions\n",
    "    for book_id in user_books:\n",
    "        query_book_features = X[X.index == book_id]\n",
    "        \n",
    "        # Check if any matching books are found\n",
    "        if not query_book_features.empty:\n",
    "            prediction = predict_like(query_book_features)\n",
    "            book_title = bk.loc[bk['bookId'] == book_id, 'Title'].values\n",
    "            if book_title:\n",
    "                print(f\"Prediction for book '{book_title[0]}' by user '{user_id}': {prediction}\")\n",
    "            else:\n",
    "                print(f\"No title found for book ID '{book_id}'\")\n",
    "        else:\n",
    "            print(f\"No matching book found for user '{user_id}' and book ID '{book_id}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d02b96892423aeb1aac89d3107554f329edbc6ff428593f945b88a67dec7f0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
